{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restricted_Boltzmann_Machines_Ange_Valli.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1o9SgZVD7TIvfBBsowJ4bmpRHoS_MkCuQ",
      "authorship_tag": "ABX9TyM5omuiaIz1YgeYogegvHRG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwlx5qpdU0pW"
      },
      "source": [
        "See original notebook : https://colab.research.google.com/drive/1hp14O7a8KYpW2e-hZOncjGeAgWJiv9Kk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yajur1TIUlYo"
      },
      "source": [
        "# Restricted Boltzmann Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCRs9dLwUazY"
      },
      "source": [
        "The aim of this project is to construct a Restricted Boltzmann Machine with Constraint-Divergence 1 algorithm\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1cfSpd35npnSvmQcsPUY8FLT096Y8JIGs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0lmH90ydeKr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_iRQB5zda8F"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from scipy.io.matlab import mio\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYu23OwkPf0K"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7sF5w_LTlEc"
      },
      "source": [
        "The datasets considered in this project are the following :\n",
        "- Binary Alpha Digits, which contains binary images of size 20*16 representing numbers from 0 to 9 and upper letters from A to Z.\n",
        "- MNIST which contains 8-bits images in shades of grey of size 28*28 representing numbers from 0 to 9\n",
        "- USPS which contains 8-bits images in shades of grey of size 16*16 representing numbers from 0 to 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3eDb_vAPh7a"
      },
      "source": [
        "!wget https://cs.nyu.edu/~roweis/data/mnist_all.mat\n",
        "!wget https://cs.nyu.edu/~roweis/data/usps_all.mat\n",
        "!wget https://cs.nyu.edu/~roweis/data/binaryalphadigs.mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd429s33dS-9"
      },
      "source": [
        "# Class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIUxnbNxc9Ua"
      },
      "source": [
        "class RBM:\n",
        "    \"\"\"Random Boltzman Machine\"\"\"\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        self.W = 0.01*np.random.randn(input_size,output_size)\n",
        "        self.a = np.zeros((1,input_size))\n",
        "        self.b = np.zeros((1,output_size))\n",
        "        self.p = input_size\n",
        "        self.q = output_size\n",
        "\n",
        "    def input_output(self, v):\n",
        "        # size = (v.shape[0],self.q)\n",
        "        return sigmoid(v@self.W + self.b)\n",
        "\n",
        "    def output_input(self,h):\n",
        "        # size = (h.shape[0],self.p)\n",
        "        return sigmoid(h@self.W.T + self.a)\n",
        "\n",
        "    def train(self, x, epochs: int=5000, lr: float =1e-1, minibatchsize: int = 10):\n",
        "        \"\"\"\n",
        "        Training of the Restricted Boltzmann Machine on data x\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # shuffle data\n",
        "            np.random.shuffle(x)\n",
        "            n = x.shape[0] - data.shape[0]%minibatchsize # for n to be a multiple of minibatchsize\n",
        "            for batch in range(0,n,minibatchsize):\n",
        "                data_batch = x[batch:batch+minibatchsize,:].copy()\n",
        "                v0 = data_batch.copy()\n",
        "                proba_h0 = self.input_output(v0)\n",
        "                # random sample of the output, obtained from the data, following a Bernouilli distribution\n",
        "                h0 = (np.random.rand(minibatchsize,self.q) < proba_h0).astype(int)\n",
        "                proba_v1 = self.output_input(h0)\n",
        "                v1 = (np.random.rand(minibatchsize,self.p) < proba_v1).astype(int)\n",
        "                proba_h1 = self.input_output(v1)\n",
        "                # compute the loss function\n",
        "                L = np.sum(np.power(v0-v1,2))/minibatchsize\n",
        "                # starting update\n",
        "                # averaging the update over the whole minibatch\n",
        "                dL_W = (data_batch.T@proba_h0-v1.T@proba_h1)/minibatchsize\n",
        "                dL_a = np.ones(shape=(1,minibatchsize))@(v0-v1)/minibatchsize\n",
        "                dL_b = np.ones(shape=(1,minibatchsize))@(proba_h0-proba_h1)/minibatchsize\n",
        "                self.W+=lr*dL_W\n",
        "                self.a+=lr*dL_a\n",
        "                self.b+=lr*dL_b\n",
        "            if epoch%10==0:\n",
        "                print(\"  Loss : %f  |  epoch %d/%d\" %(L, epoch+1,epochs))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "def generate_image(RBM_instance, iter_gibbs, nb_img):\n",
        "    \"\"\"\n",
        "    Generate an image from a Restricted Boltzmann Machine instance using a Gibbs sampler\n",
        "    \"\"\"\n",
        "    d=np.zeros((width,1))\n",
        "    space=np.zeros((width,1))#separate the generated images with a blank column\n",
        "    for i in range(nb_img):\n",
        "        plt.figure()\n",
        "        v = (np.random.rand(RBM_instance.p) < 0.5)*1\n",
        "        for j in range(iter_gibbs):\n",
        "            h = (np.random.rand(RBM_instance.q) < RBM_instance.input_output(v))*1\n",
        "            v = (np.random.rand(RBM_instance.p) < RBM_instance.output_input(h))*1\n",
        "        \n",
        "        v=v.reshape(width,height)\n",
        "        d=np.concatenate((d,v),axis=1)\n",
        "        d=np.concatenate((d,space),axis=1)\n",
        "    plt.imshow(d)\n",
        "\n",
        "\n",
        "def read_alpha_digits(caractères):\n",
        "    \"\"\"\n",
        "    Read data from dataset and flatten the samples\n",
        "    \"\"\"\n",
        "    matrice = []\n",
        "    for caractère in caractères:\n",
        "        for example in fichier['dat'][caractère]:\n",
        "            matrice.append(example.reshape(width*height)) # Reshape with size 20*16\n",
        "    return np.array(matrice)\n",
        "\n",
        "\n",
        "def read_mnist(caractères):\n",
        "    \"\"\"\n",
        "    Read data from dataset and flatten the samples\n",
        "    \"\"\"\n",
        "    matrice, samples = [], []\n",
        "    for caractère in caractères:\n",
        "        for example in fichier['train'+str(caractère)]:\n",
        "            example = (example > 255/2)*1  # Binarization of data\n",
        "            samples.append(example.reshape(28*28)) # Reshape with size 28*28\n",
        "        samples = random.sample(samples, 250) # Selecting only 250 samples for a better computation time\n",
        "        matrice += samples\n",
        "    return np.array(matrice)\n",
        "\n",
        "\n",
        "def read_usps(caractères):\n",
        "    \"\"\"\n",
        "    Read data from dataset and flatten the samples\n",
        "    \"\"\"\n",
        "    matrice, samples = [], []\n",
        "    data = np.swapaxes(fichier['data'], 0, 2)\n",
        "    for caractère in caractères:\n",
        "        for example in data[caractère]:\n",
        "            example = (example > 255/2)*1  # Binarization of data\n",
        "            samples.append(example.reshape(16*16))  # Reshape with size 16*16\n",
        "        samples = random.sample(samples, 250) # Selecting only 250 samples for a better computation time\n",
        "        matrice += samples\n",
        "    return np.array(matrice)\n",
        "\n",
        "def loss(v,v1):\n",
        "    return np.sum(np.power((v-v1),2))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t76Q5_5edoRZ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjI0I2pSdwT3"
      },
      "source": [
        "# Dataset selection\n",
        "dataset = 'mnist' # 'usps' 'binaryalphadigits'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZCG51olgdslI",
        "outputId": "347be06b-01fa-4503-bcc5-0ac9a5f878d6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if dataset == \"binaryalphadigs\":\n",
        "        width, height = 20, 16\n",
        "        fichier = mio.loadmat(\"./binaryalphadigs\")\n",
        "        data=read_alpha_digits([10,11,12,13,14])\n",
        "        h=RBM(320,200) # RBM of size 320 = 20*16\n",
        "        h.train(data)\n",
        "\n",
        "    elif dataset == \"mnist\":\n",
        "        width, height = 28, 28\n",
        "        fichier = mio.loadmat(\"./mnist_all\")\n",
        "        data=read_mnist([0,1,2])\n",
        "        h=RBM(784,200) # RBM of size 784 = 28*28\n",
        "        h.train(data)\n",
        "\n",
        "    elif dataset == \"usps\":\n",
        "        width, height = 16, 16\n",
        "        fichier = mio.loadmat(\"./usps_all\")\n",
        "        data=read_usps([2,3,4,5,6])\n",
        "        h=RBM(256,200) # RBM of size 256 = 16*16\n",
        "        h.train(data)\n",
        "    \n",
        "    generate_image(h,5000,10)\n",
        "\n",
        "    #pick a random sample and verify the loss between the sample and the reconstructed image\n",
        "    #i=random.randint(0,data.shape[0])\n",
        "\n",
        "    #character=data[i]\n",
        "    #reconstructed_char=h.output_input(h.input_output(character))\n",
        "    #print(loss(character,reconstructed_char))\n",
        "    #character=character.reshape(width,height)\n",
        "    #reconstructed_char=reconstructed_char.reshape(width,height)\n",
        "    #plt.figure()\n",
        "    #plt.imshow(np.concatenate((character,reconstructed_char),axis=1))\n",
        "    #plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loss : 87.500000  |  epoch 1/5000\n",
            "  Loss : 46.100000  |  epoch 11/5000\n",
            "  Loss : 31.500000  |  epoch 21/5000\n",
            "  Loss : 34.800000  |  epoch 31/5000\n",
            "  Loss : 27.500000  |  epoch 41/5000\n",
            "  Loss : 15.100000  |  epoch 51/5000\n",
            "  Loss : 34.700000  |  epoch 61/5000\n",
            "  Loss : 25.800000  |  epoch 71/5000\n",
            "  Loss : 15.800000  |  epoch 81/5000\n",
            "  Loss : 16.200000  |  epoch 91/5000\n",
            "  Loss : 16.200000  |  epoch 101/5000\n",
            "  Loss : 12.400000  |  epoch 111/5000\n",
            "  Loss : 13.200000  |  epoch 121/5000\n",
            "  Loss : 15.600000  |  epoch 131/5000\n",
            "  Loss : 9.600000  |  epoch 141/5000\n",
            "  Loss : 11.700000  |  epoch 151/5000\n",
            "  Loss : 9.900000  |  epoch 161/5000\n",
            "  Loss : 10.400000  |  epoch 171/5000\n",
            "  Loss : 10.600000  |  epoch 181/5000\n",
            "  Loss : 13.800000  |  epoch 191/5000\n",
            "  Loss : 11.900000  |  epoch 201/5000\n",
            "  Loss : 11.400000  |  epoch 211/5000\n",
            "  Loss : 7.000000  |  epoch 221/5000\n",
            "  Loss : 7.600000  |  epoch 231/5000\n",
            "  Loss : 7.700000  |  epoch 241/5000\n",
            "  Loss : 5.100000  |  epoch 251/5000\n",
            "  Loss : 9.600000  |  epoch 261/5000\n",
            "  Loss : 8.100000  |  epoch 271/5000\n",
            "  Loss : 7.600000  |  epoch 281/5000\n",
            "  Loss : 5.300000  |  epoch 291/5000\n",
            "  Loss : 5.800000  |  epoch 301/5000\n",
            "  Loss : 8.200000  |  epoch 311/5000\n",
            "  Loss : 5.300000  |  epoch 321/5000\n",
            "  Loss : 7.600000  |  epoch 331/5000\n",
            "  Loss : 7.000000  |  epoch 341/5000\n",
            "  Loss : 7.500000  |  epoch 351/5000\n",
            "  Loss : 6.600000  |  epoch 361/5000\n",
            "  Loss : 7.100000  |  epoch 371/5000\n",
            "  Loss : 2.200000  |  epoch 381/5000\n",
            "  Loss : 5.500000  |  epoch 391/5000\n",
            "  Loss : 4.300000  |  epoch 401/5000\n",
            "  Loss : 8.200000  |  epoch 411/5000\n",
            "  Loss : 7.000000  |  epoch 421/5000\n",
            "  Loss : 3.900000  |  epoch 431/5000\n",
            "  Loss : 5.000000  |  epoch 441/5000\n",
            "  Loss : 5.200000  |  epoch 451/5000\n",
            "  Loss : 2.600000  |  epoch 461/5000\n",
            "  Loss : 4.200000  |  epoch 471/5000\n",
            "  Loss : 4.600000  |  epoch 481/5000\n",
            "  Loss : 6.400000  |  epoch 491/5000\n",
            "  Loss : 6.800000  |  epoch 501/5000\n",
            "  Loss : 5.600000  |  epoch 511/5000\n",
            "  Loss : 4.600000  |  epoch 521/5000\n",
            "  Loss : 4.000000  |  epoch 531/5000\n",
            "  Loss : 3.700000  |  epoch 541/5000\n",
            "  Loss : 4.600000  |  epoch 551/5000\n",
            "  Loss : 3.600000  |  epoch 561/5000\n",
            "  Loss : 3.300000  |  epoch 571/5000\n",
            "  Loss : 4.800000  |  epoch 581/5000\n",
            "  Loss : 3.500000  |  epoch 591/5000\n",
            "  Loss : 3.100000  |  epoch 601/5000\n",
            "  Loss : 2.900000  |  epoch 611/5000\n",
            "  Loss : 2.800000  |  epoch 621/5000\n",
            "  Loss : 5.400000  |  epoch 631/5000\n",
            "  Loss : 3.500000  |  epoch 641/5000\n",
            "  Loss : 4.000000  |  epoch 651/5000\n",
            "  Loss : 1.600000  |  epoch 661/5000\n",
            "  Loss : 2.600000  |  epoch 671/5000\n",
            "  Loss : 4.600000  |  epoch 681/5000\n",
            "  Loss : 2.100000  |  epoch 691/5000\n",
            "  Loss : 2.600000  |  epoch 701/5000\n",
            "  Loss : 3.200000  |  epoch 711/5000\n",
            "  Loss : 3.300000  |  epoch 721/5000\n",
            "  Loss : 2.700000  |  epoch 731/5000\n",
            "  Loss : 3.100000  |  epoch 741/5000\n",
            "  Loss : 1.400000  |  epoch 751/5000\n",
            "  Loss : 4.900000  |  epoch 761/5000\n",
            "  Loss : 3.700000  |  epoch 771/5000\n",
            "  Loss : 2.800000  |  epoch 781/5000\n",
            "  Loss : 3.200000  |  epoch 791/5000\n",
            "  Loss : 2.300000  |  epoch 801/5000\n",
            "  Loss : 2.200000  |  epoch 811/5000\n",
            "  Loss : 2.300000  |  epoch 821/5000\n",
            "  Loss : 2.200000  |  epoch 831/5000\n",
            "  Loss : 2.200000  |  epoch 841/5000\n",
            "  Loss : 2.300000  |  epoch 851/5000\n",
            "  Loss : 3.600000  |  epoch 861/5000\n",
            "  Loss : 2.500000  |  epoch 871/5000\n",
            "  Loss : 2.800000  |  epoch 881/5000\n",
            "  Loss : 1.800000  |  epoch 891/5000\n",
            "  Loss : 1.200000  |  epoch 901/5000\n",
            "  Loss : 2.800000  |  epoch 911/5000\n",
            "  Loss : 3.500000  |  epoch 921/5000\n",
            "  Loss : 4.000000  |  epoch 931/5000\n",
            "  Loss : 2.400000  |  epoch 941/5000\n",
            "  Loss : 3.100000  |  epoch 951/5000\n",
            "  Loss : 2.600000  |  epoch 961/5000\n",
            "  Loss : 1.500000  |  epoch 971/5000\n",
            "  Loss : 2.000000  |  epoch 981/5000\n",
            "  Loss : 1.600000  |  epoch 991/5000\n",
            "  Loss : 1.900000  |  epoch 1001/5000\n",
            "  Loss : 2.000000  |  epoch 1011/5000\n",
            "  Loss : 1.800000  |  epoch 1021/5000\n",
            "  Loss : 2.400000  |  epoch 1031/5000\n",
            "  Loss : 2.000000  |  epoch 1041/5000\n",
            "  Loss : 2.200000  |  epoch 1051/5000\n",
            "  Loss : 3.700000  |  epoch 1061/5000\n",
            "  Loss : 1.600000  |  epoch 1071/5000\n",
            "  Loss : 1.900000  |  epoch 1081/5000\n",
            "  Loss : 1.700000  |  epoch 1091/5000\n",
            "  Loss : 1.300000  |  epoch 1101/5000\n",
            "  Loss : 2.100000  |  epoch 1111/5000\n",
            "  Loss : 1.100000  |  epoch 1121/5000\n",
            "  Loss : 0.900000  |  epoch 1131/5000\n",
            "  Loss : 2.600000  |  epoch 1141/5000\n",
            "  Loss : 1.800000  |  epoch 1151/5000\n",
            "  Loss : 2.000000  |  epoch 1161/5000\n",
            "  Loss : 1.100000  |  epoch 1171/5000\n",
            "  Loss : 2.200000  |  epoch 1181/5000\n",
            "  Loss : 2.500000  |  epoch 1191/5000\n",
            "  Loss : 1.300000  |  epoch 1201/5000\n",
            "  Loss : 2.200000  |  epoch 1211/5000\n",
            "  Loss : 1.900000  |  epoch 1221/5000\n",
            "  Loss : 1.200000  |  epoch 1231/5000\n",
            "  Loss : 1.000000  |  epoch 1241/5000\n",
            "  Loss : 1.700000  |  epoch 1251/5000\n",
            "  Loss : 3.400000  |  epoch 1261/5000\n",
            "  Loss : 2.100000  |  epoch 1271/5000\n",
            "  Loss : 1.500000  |  epoch 1281/5000\n",
            "  Loss : 1.300000  |  epoch 1291/5000\n",
            "  Loss : 0.900000  |  epoch 1301/5000\n",
            "  Loss : 1.500000  |  epoch 1311/5000\n",
            "  Loss : 2.100000  |  epoch 1321/5000\n",
            "  Loss : 2.000000  |  epoch 1331/5000\n",
            "  Loss : 1.300000  |  epoch 1341/5000\n",
            "  Loss : 2.600000  |  epoch 1351/5000\n",
            "  Loss : 0.900000  |  epoch 1361/5000\n",
            "  Loss : 1.500000  |  epoch 1371/5000\n",
            "  Loss : 1.700000  |  epoch 1381/5000\n",
            "  Loss : 1.200000  |  epoch 1391/5000\n",
            "  Loss : 2.000000  |  epoch 1401/5000\n",
            "  Loss : 2.000000  |  epoch 1411/5000\n",
            "  Loss : 2.100000  |  epoch 1421/5000\n",
            "  Loss : 1.800000  |  epoch 1431/5000\n",
            "  Loss : 1.800000  |  epoch 1441/5000\n",
            "  Loss : 1.300000  |  epoch 1451/5000\n",
            "  Loss : 2.100000  |  epoch 1461/5000\n",
            "  Loss : 1.900000  |  epoch 1471/5000\n",
            "  Loss : 1.000000  |  epoch 1481/5000\n",
            "  Loss : 1.500000  |  epoch 1491/5000\n",
            "  Loss : 2.200000  |  epoch 1501/5000\n",
            "  Loss : 1.500000  |  epoch 1511/5000\n",
            "  Loss : 0.800000  |  epoch 1521/5000\n",
            "  Loss : 0.300000  |  epoch 1531/5000\n",
            "  Loss : 1.200000  |  epoch 1541/5000\n",
            "  Loss : 0.500000  |  epoch 1551/5000\n",
            "  Loss : 1.800000  |  epoch 1561/5000\n",
            "  Loss : 1.200000  |  epoch 1571/5000\n",
            "  Loss : 2.300000  |  epoch 1581/5000\n",
            "  Loss : 1.700000  |  epoch 1591/5000\n",
            "  Loss : 1.500000  |  epoch 1601/5000\n",
            "  Loss : 1.400000  |  epoch 1611/5000\n",
            "  Loss : 1.400000  |  epoch 1621/5000\n",
            "  Loss : 0.400000  |  epoch 1631/5000\n",
            "  Loss : 0.800000  |  epoch 1641/5000\n",
            "  Loss : 1.000000  |  epoch 1651/5000\n",
            "  Loss : 1.300000  |  epoch 1661/5000\n",
            "  Loss : 1.400000  |  epoch 1671/5000\n",
            "  Loss : 0.700000  |  epoch 1681/5000\n",
            "  Loss : 1.600000  |  epoch 1691/5000\n",
            "  Loss : 1.200000  |  epoch 1701/5000\n",
            "  Loss : 1.100000  |  epoch 1711/5000\n",
            "  Loss : 1.500000  |  epoch 1721/5000\n",
            "  Loss : 3.100000  |  epoch 1731/5000\n",
            "  Loss : 2.200000  |  epoch 1741/5000\n",
            "  Loss : 0.400000  |  epoch 1751/5000\n",
            "  Loss : 1.100000  |  epoch 1761/5000\n",
            "  Loss : 1.500000  |  epoch 1771/5000\n",
            "  Loss : 0.800000  |  epoch 1781/5000\n",
            "  Loss : 0.700000  |  epoch 1791/5000\n",
            "  Loss : 1.600000  |  epoch 1801/5000\n",
            "  Loss : 1.400000  |  epoch 1811/5000\n",
            "  Loss : 1.000000  |  epoch 1821/5000\n",
            "  Loss : 1.000000  |  epoch 1831/5000\n",
            "  Loss : 1.100000  |  epoch 1841/5000\n",
            "  Loss : 1.500000  |  epoch 1851/5000\n",
            "  Loss : 1.300000  |  epoch 1861/5000\n",
            "  Loss : 1.400000  |  epoch 1871/5000\n",
            "  Loss : 1.400000  |  epoch 1881/5000\n",
            "  Loss : 1.200000  |  epoch 1891/5000\n",
            "  Loss : 0.500000  |  epoch 1901/5000\n",
            "  Loss : 1.400000  |  epoch 1911/5000\n",
            "  Loss : 1.500000  |  epoch 1921/5000\n",
            "  Loss : 0.200000  |  epoch 1931/5000\n",
            "  Loss : 1.700000  |  epoch 1941/5000\n",
            "  Loss : 1.000000  |  epoch 1951/5000\n",
            "  Loss : 0.700000  |  epoch 1961/5000\n",
            "  Loss : 0.800000  |  epoch 1971/5000\n",
            "  Loss : 0.600000  |  epoch 1981/5000\n",
            "  Loss : 0.800000  |  epoch 1991/5000\n",
            "  Loss : 0.600000  |  epoch 2001/5000\n",
            "  Loss : 1.400000  |  epoch 2011/5000\n",
            "  Loss : 1.500000  |  epoch 2021/5000\n",
            "  Loss : 1.100000  |  epoch 2031/5000\n",
            "  Loss : 1.500000  |  epoch 2041/5000\n",
            "  Loss : 0.700000  |  epoch 2051/5000\n",
            "  Loss : 1.200000  |  epoch 2061/5000\n",
            "  Loss : 1.400000  |  epoch 2071/5000\n",
            "  Loss : 1.200000  |  epoch 2081/5000\n",
            "  Loss : 0.800000  |  epoch 2091/5000\n",
            "  Loss : 1.000000  |  epoch 2101/5000\n",
            "  Loss : 1.500000  |  epoch 2111/5000\n",
            "  Loss : 0.800000  |  epoch 2121/5000\n",
            "  Loss : 1.000000  |  epoch 2131/5000\n",
            "  Loss : 1.000000  |  epoch 2141/5000\n",
            "  Loss : 1.100000  |  epoch 2151/5000\n",
            "  Loss : 0.500000  |  epoch 2161/5000\n",
            "  Loss : 0.700000  |  epoch 2171/5000\n",
            "  Loss : 1.100000  |  epoch 2181/5000\n",
            "  Loss : 0.600000  |  epoch 2191/5000\n",
            "  Loss : 2.000000  |  epoch 2201/5000\n",
            "  Loss : 0.700000  |  epoch 2211/5000\n",
            "  Loss : 1.200000  |  epoch 2221/5000\n",
            "  Loss : 1.000000  |  epoch 2231/5000\n",
            "  Loss : 1.300000  |  epoch 2241/5000\n",
            "  Loss : 0.800000  |  epoch 2251/5000\n",
            "  Loss : 0.900000  |  epoch 2261/5000\n",
            "  Loss : 0.800000  |  epoch 2271/5000\n",
            "  Loss : 1.300000  |  epoch 2281/5000\n",
            "  Loss : 0.900000  |  epoch 2291/5000\n",
            "  Loss : 0.600000  |  epoch 2301/5000\n",
            "  Loss : 1.000000  |  epoch 2311/5000\n",
            "  Loss : 1.200000  |  epoch 2321/5000\n",
            "  Loss : 0.900000  |  epoch 2331/5000\n",
            "  Loss : 1.100000  |  epoch 2341/5000\n",
            "  Loss : 0.700000  |  epoch 2351/5000\n",
            "  Loss : 1.300000  |  epoch 2361/5000\n",
            "  Loss : 0.600000  |  epoch 2371/5000\n",
            "  Loss : 1.200000  |  epoch 2381/5000\n",
            "  Loss : 1.200000  |  epoch 2391/5000\n",
            "  Loss : 1.100000  |  epoch 2401/5000\n",
            "  Loss : 0.900000  |  epoch 2411/5000\n",
            "  Loss : 0.200000  |  epoch 2421/5000\n",
            "  Loss : 0.600000  |  epoch 2431/5000\n",
            "  Loss : 0.300000  |  epoch 2441/5000\n",
            "  Loss : 0.300000  |  epoch 2451/5000\n",
            "  Loss : 0.600000  |  epoch 2461/5000\n",
            "  Loss : 1.100000  |  epoch 2471/5000\n",
            "  Loss : 0.800000  |  epoch 2481/5000\n",
            "  Loss : 1.000000  |  epoch 2491/5000\n",
            "  Loss : 1.500000  |  epoch 2501/5000\n",
            "  Loss : 1.500000  |  epoch 2511/5000\n",
            "  Loss : 1.200000  |  epoch 2521/5000\n",
            "  Loss : 0.600000  |  epoch 2531/5000\n",
            "  Loss : 0.500000  |  epoch 2541/5000\n",
            "  Loss : 1.500000  |  epoch 2551/5000\n",
            "  Loss : 0.900000  |  epoch 2561/5000\n",
            "  Loss : 1.500000  |  epoch 2571/5000\n",
            "  Loss : 1.200000  |  epoch 2581/5000\n",
            "  Loss : 1.300000  |  epoch 2591/5000\n",
            "  Loss : 0.800000  |  epoch 2601/5000\n",
            "  Loss : 0.300000  |  epoch 2611/5000\n",
            "  Loss : 1.500000  |  epoch 2621/5000\n",
            "  Loss : 1.000000  |  epoch 2631/5000\n",
            "  Loss : 0.800000  |  epoch 2641/5000\n",
            "  Loss : 1.000000  |  epoch 2651/5000\n",
            "  Loss : 0.400000  |  epoch 2661/5000\n",
            "  Loss : 0.400000  |  epoch 2671/5000\n",
            "  Loss : 1.000000  |  epoch 2681/5000\n",
            "  Loss : 0.600000  |  epoch 2691/5000\n",
            "  Loss : 0.400000  |  epoch 2701/5000\n",
            "  Loss : 0.500000  |  epoch 2711/5000\n",
            "  Loss : 1.300000  |  epoch 2721/5000\n",
            "  Loss : 1.200000  |  epoch 2731/5000\n",
            "  Loss : 0.900000  |  epoch 2741/5000\n",
            "  Loss : 0.900000  |  epoch 2751/5000\n",
            "  Loss : 1.000000  |  epoch 2761/5000\n",
            "  Loss : 0.800000  |  epoch 2771/5000\n",
            "  Loss : 1.400000  |  epoch 2781/5000\n",
            "  Loss : 0.600000  |  epoch 2791/5000\n",
            "  Loss : 0.300000  |  epoch 2801/5000\n",
            "  Loss : 0.300000  |  epoch 2811/5000\n",
            "  Loss : 1.300000  |  epoch 2821/5000\n",
            "  Loss : 0.500000  |  epoch 2831/5000\n",
            "  Loss : 0.700000  |  epoch 2841/5000\n",
            "  Loss : 0.800000  |  epoch 2851/5000\n",
            "  Loss : 1.000000  |  epoch 2861/5000\n",
            "  Loss : 0.700000  |  epoch 2871/5000\n",
            "  Loss : 1.100000  |  epoch 2881/5000\n",
            "  Loss : 0.600000  |  epoch 2891/5000\n",
            "  Loss : 1.000000  |  epoch 2901/5000\n",
            "  Loss : 0.300000  |  epoch 2911/5000\n",
            "  Loss : 0.600000  |  epoch 2921/5000\n",
            "  Loss : 0.300000  |  epoch 2931/5000\n",
            "  Loss : 0.500000  |  epoch 2941/5000\n",
            "  Loss : 0.400000  |  epoch 2951/5000\n",
            "  Loss : 1.700000  |  epoch 2961/5000\n",
            "  Loss : 1.000000  |  epoch 2971/5000\n",
            "  Loss : 0.800000  |  epoch 2981/5000\n",
            "  Loss : 0.700000  |  epoch 2991/5000\n",
            "  Loss : 0.700000  |  epoch 3001/5000\n",
            "  Loss : 0.100000  |  epoch 3011/5000\n",
            "  Loss : 1.100000  |  epoch 3021/5000\n",
            "  Loss : 1.300000  |  epoch 3031/5000\n",
            "  Loss : 0.600000  |  epoch 3041/5000\n",
            "  Loss : 0.400000  |  epoch 3051/5000\n",
            "  Loss : 0.700000  |  epoch 3061/5000\n",
            "  Loss : 1.100000  |  epoch 3071/5000\n",
            "  Loss : 0.600000  |  epoch 3081/5000\n",
            "  Loss : 0.300000  |  epoch 3091/5000\n",
            "  Loss : 0.300000  |  epoch 3101/5000\n",
            "  Loss : 0.300000  |  epoch 3111/5000\n",
            "  Loss : 0.800000  |  epoch 3121/5000\n",
            "  Loss : 0.600000  |  epoch 3131/5000\n",
            "  Loss : 0.400000  |  epoch 3141/5000\n",
            "  Loss : 0.800000  |  epoch 3151/5000\n",
            "  Loss : 0.500000  |  epoch 3161/5000\n",
            "  Loss : 0.400000  |  epoch 3171/5000\n",
            "  Loss : 0.800000  |  epoch 3181/5000\n",
            "  Loss : 0.300000  |  epoch 3191/5000\n",
            "  Loss : 0.700000  |  epoch 3201/5000\n",
            "  Loss : 0.400000  |  epoch 3211/5000\n",
            "  Loss : 0.800000  |  epoch 3221/5000\n",
            "  Loss : 0.400000  |  epoch 3231/5000\n",
            "  Loss : 0.500000  |  epoch 3241/5000\n",
            "  Loss : 0.800000  |  epoch 3251/5000\n",
            "  Loss : 0.800000  |  epoch 3261/5000\n",
            "  Loss : 0.700000  |  epoch 3271/5000\n",
            "  Loss : 0.900000  |  epoch 3281/5000\n",
            "  Loss : 0.700000  |  epoch 3291/5000\n",
            "  Loss : 0.300000  |  epoch 3301/5000\n",
            "  Loss : 0.300000  |  epoch 3311/5000\n",
            "  Loss : 0.500000  |  epoch 3321/5000\n",
            "  Loss : 1.700000  |  epoch 3331/5000\n",
            "  Loss : 1.000000  |  epoch 3341/5000\n",
            "  Loss : 1.100000  |  epoch 3351/5000\n",
            "  Loss : 0.600000  |  epoch 3361/5000\n",
            "  Loss : 0.600000  |  epoch 3371/5000\n",
            "  Loss : 0.300000  |  epoch 3381/5000\n",
            "  Loss : 0.700000  |  epoch 3391/5000\n",
            "  Loss : 0.500000  |  epoch 3401/5000\n",
            "  Loss : 0.600000  |  epoch 3411/5000\n",
            "  Loss : 0.800000  |  epoch 3421/5000\n",
            "  Loss : 0.500000  |  epoch 3431/5000\n",
            "  Loss : 0.700000  |  epoch 3441/5000\n",
            "  Loss : 0.300000  |  epoch 3451/5000\n",
            "  Loss : 1.000000  |  epoch 3461/5000\n",
            "  Loss : 0.900000  |  epoch 3471/5000\n",
            "  Loss : 0.700000  |  epoch 3481/5000\n",
            "  Loss : 1.200000  |  epoch 3491/5000\n",
            "  Loss : 0.600000  |  epoch 3501/5000\n",
            "  Loss : 0.800000  |  epoch 3511/5000\n",
            "  Loss : 0.300000  |  epoch 3521/5000\n",
            "  Loss : 0.700000  |  epoch 3531/5000\n",
            "  Loss : 0.900000  |  epoch 3541/5000\n",
            "  Loss : 0.700000  |  epoch 3551/5000\n",
            "  Loss : 1.500000  |  epoch 3561/5000\n",
            "  Loss : 1.200000  |  epoch 3571/5000\n",
            "  Loss : 0.800000  |  epoch 3581/5000\n",
            "  Loss : 0.500000  |  epoch 3591/5000\n",
            "  Loss : 1.000000  |  epoch 3601/5000\n",
            "  Loss : 0.600000  |  epoch 3611/5000\n",
            "  Loss : 1.100000  |  epoch 3621/5000\n",
            "  Loss : 0.400000  |  epoch 3631/5000\n",
            "  Loss : 1.200000  |  epoch 3641/5000\n",
            "  Loss : 0.600000  |  epoch 3651/5000\n",
            "  Loss : 0.800000  |  epoch 3661/5000\n",
            "  Loss : 0.500000  |  epoch 3671/5000\n",
            "  Loss : 0.700000  |  epoch 3681/5000\n",
            "  Loss : 0.700000  |  epoch 3691/5000\n",
            "  Loss : 1.400000  |  epoch 3701/5000\n",
            "  Loss : 1.000000  |  epoch 3711/5000\n",
            "  Loss : 0.500000  |  epoch 3721/5000\n",
            "  Loss : 0.800000  |  epoch 3731/5000\n",
            "  Loss : 0.700000  |  epoch 3741/5000\n",
            "  Loss : 0.900000  |  epoch 3751/5000\n",
            "  Loss : 0.800000  |  epoch 3761/5000\n",
            "  Loss : 0.700000  |  epoch 3771/5000\n",
            "  Loss : 0.900000  |  epoch 3781/5000\n",
            "  Loss : 0.900000  |  epoch 3791/5000\n",
            "  Loss : 0.500000  |  epoch 3801/5000\n",
            "  Loss : 0.600000  |  epoch 3811/5000\n",
            "  Loss : 0.800000  |  epoch 3821/5000\n",
            "  Loss : 0.500000  |  epoch 3831/5000\n",
            "  Loss : 0.200000  |  epoch 3841/5000\n",
            "  Loss : 0.100000  |  epoch 3851/5000\n",
            "  Loss : 0.800000  |  epoch 3861/5000\n",
            "  Loss : 0.600000  |  epoch 3871/5000\n",
            "  Loss : 0.800000  |  epoch 3881/5000\n",
            "  Loss : 0.800000  |  epoch 3891/5000\n",
            "  Loss : 0.300000  |  epoch 3901/5000\n",
            "  Loss : 0.300000  |  epoch 3911/5000\n",
            "  Loss : 0.400000  |  epoch 3921/5000\n",
            "  Loss : 0.700000  |  epoch 3931/5000\n",
            "  Loss : 0.200000  |  epoch 3941/5000\n",
            "  Loss : 0.600000  |  epoch 3951/5000\n",
            "  Loss : 0.900000  |  epoch 3961/5000\n",
            "  Loss : 0.600000  |  epoch 3971/5000\n",
            "  Loss : 0.700000  |  epoch 3981/5000\n",
            "  Loss : 0.700000  |  epoch 3991/5000\n",
            "  Loss : 0.900000  |  epoch 4001/5000\n",
            "  Loss : 0.900000  |  epoch 4011/5000\n",
            "  Loss : 0.400000  |  epoch 4021/5000\n",
            "  Loss : 0.100000  |  epoch 4031/5000\n",
            "  Loss : 0.300000  |  epoch 4041/5000\n",
            "  Loss : 1.000000  |  epoch 4051/5000\n",
            "  Loss : 0.400000  |  epoch 4061/5000\n",
            "  Loss : 0.900000  |  epoch 4071/5000\n",
            "  Loss : 0.600000  |  epoch 4081/5000\n",
            "  Loss : 0.900000  |  epoch 4091/5000\n",
            "  Loss : 1.200000  |  epoch 4101/5000\n",
            "  Loss : 0.100000  |  epoch 4111/5000\n",
            "  Loss : 0.400000  |  epoch 4121/5000\n",
            "  Loss : 0.400000  |  epoch 4131/5000\n",
            "  Loss : 0.300000  |  epoch 4141/5000\n",
            "  Loss : 0.700000  |  epoch 4151/5000\n",
            "  Loss : 0.200000  |  epoch 4161/5000\n",
            "  Loss : 0.500000  |  epoch 4171/5000\n",
            "  Loss : 0.300000  |  epoch 4181/5000\n",
            "  Loss : 0.400000  |  epoch 4191/5000\n",
            "  Loss : 0.200000  |  epoch 4201/5000\n",
            "  Loss : 0.300000  |  epoch 4211/5000\n",
            "  Loss : 0.400000  |  epoch 4221/5000\n",
            "  Loss : 0.500000  |  epoch 4231/5000\n",
            "  Loss : 0.500000  |  epoch 4241/5000\n",
            "  Loss : 0.100000  |  epoch 4251/5000\n",
            "  Loss : 0.200000  |  epoch 4261/5000\n",
            "  Loss : 0.400000  |  epoch 4271/5000\n",
            "  Loss : 0.200000  |  epoch 4281/5000\n",
            "  Loss : 0.500000  |  epoch 4291/5000\n",
            "  Loss : 0.700000  |  epoch 4301/5000\n",
            "  Loss : 0.300000  |  epoch 4311/5000\n",
            "  Loss : 0.400000  |  epoch 4321/5000\n",
            "  Loss : 0.100000  |  epoch 4331/5000\n",
            "  Loss : 0.500000  |  epoch 4341/5000\n",
            "  Loss : 0.700000  |  epoch 4351/5000\n",
            "  Loss : 1.000000  |  epoch 4361/5000\n",
            "  Loss : 0.600000  |  epoch 4371/5000\n",
            "  Loss : 0.500000  |  epoch 4381/5000\n",
            "  Loss : 0.800000  |  epoch 4391/5000\n",
            "  Loss : 0.300000  |  epoch 4401/5000\n",
            "  Loss : 0.800000  |  epoch 4411/5000\n",
            "  Loss : 0.300000  |  epoch 4421/5000\n",
            "  Loss : 0.400000  |  epoch 4431/5000\n",
            "  Loss : 0.600000  |  epoch 4441/5000\n",
            "  Loss : 0.600000  |  epoch 4451/5000\n",
            "  Loss : 0.500000  |  epoch 4461/5000\n",
            "  Loss : 0.300000  |  epoch 4471/5000\n",
            "  Loss : 0.500000  |  epoch 4481/5000\n",
            "  Loss : 0.500000  |  epoch 4491/5000\n",
            "  Loss : 1.000000  |  epoch 4501/5000\n",
            "  Loss : 0.600000  |  epoch 4511/5000\n",
            "  Loss : 0.800000  |  epoch 4521/5000\n",
            "  Loss : 0.200000  |  epoch 4531/5000\n",
            "  Loss : 0.400000  |  epoch 4541/5000\n",
            "  Loss : 0.900000  |  epoch 4551/5000\n",
            "  Loss : 0.300000  |  epoch 4561/5000\n",
            "  Loss : 0.600000  |  epoch 4571/5000\n",
            "  Loss : 0.700000  |  epoch 4581/5000\n",
            "  Loss : 0.400000  |  epoch 4591/5000\n",
            "  Loss : 0.800000  |  epoch 4601/5000\n",
            "  Loss : 0.500000  |  epoch 4611/5000\n",
            "  Loss : 0.600000  |  epoch 4621/5000\n",
            "  Loss : 0.600000  |  epoch 4631/5000\n",
            "  Loss : 0.400000  |  epoch 4641/5000\n",
            "  Loss : 0.500000  |  epoch 4651/5000\n",
            "  Loss : 0.300000  |  epoch 4661/5000\n",
            "  Loss : 0.500000  |  epoch 4671/5000\n",
            "  Loss : 0.500000  |  epoch 4681/5000\n",
            "  Loss : 0.300000  |  epoch 4691/5000\n",
            "  Loss : 0.200000  |  epoch 4701/5000\n",
            "  Loss : 0.800000  |  epoch 4711/5000\n",
            "  Loss : 0.700000  |  epoch 4721/5000\n",
            "  Loss : 0.500000  |  epoch 4731/5000\n",
            "  Loss : 0.500000  |  epoch 4741/5000\n",
            "  Loss : 0.300000  |  epoch 4751/5000\n",
            "  Loss : 0.300000  |  epoch 4761/5000\n",
            "  Loss : 0.700000  |  epoch 4771/5000\n",
            "  Loss : 0.400000  |  epoch 4781/5000\n",
            "  Loss : 0.600000  |  epoch 4791/5000\n",
            "  Loss : 0.700000  |  epoch 4801/5000\n",
            "  Loss : 0.200000  |  epoch 4811/5000\n",
            "  Loss : 0.300000  |  epoch 4821/5000\n",
            "  Loss : 0.200000  |  epoch 4831/5000\n",
            "  Loss : 0.600000  |  epoch 4841/5000\n",
            "  Loss : 0.700000  |  epoch 4851/5000\n",
            "  Loss : 0.800000  |  epoch 4861/5000\n",
            "  Loss : 0.800000  |  epoch 4871/5000\n",
            "  Loss : 0.500000  |  epoch 4881/5000\n",
            "  Loss : 0.600000  |  epoch 4891/5000\n",
            "  Loss : 0.100000  |  epoch 4901/5000\n",
            "  Loss : 0.500000  |  epoch 4911/5000\n",
            "  Loss : 0.500000  |  epoch 4921/5000\n",
            "  Loss : 0.200000  |  epoch 4931/5000\n",
            "  Loss : 0.700000  |  epoch 4941/5000\n",
            "  Loss : 0.400000  |  epoch 4951/5000\n",
            "  Loss : 0.800000  |  epoch 4961/5000\n",
            "  Loss : 0.800000  |  epoch 4971/5000\n",
            "  Loss : 0.100000  |  epoch 4981/5000\n",
            "  Loss : 0.100000  |  epoch 4991/5000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PONsgdZgaq0D"
      },
      "source": [
        "## Conclusions for Binary Alpha Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jJNJ1LHYkUH"
      },
      "source": [
        "The reconstruction error diminishes with the growing number of hidden units. A high number enables the algorithm to a better recognition of patterns. Here are the results for Binary Alpha Digits dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szQnZmV7Y-6Z"
      },
      "source": [
        "| Number of hidden units| Reconstruction error  |\n",
        "|-----------------------|-----------------------|\n",
        "|                10     |               44.80   |\n",
        "|               100     |               0.014   |\n",
        "|              1000     |             0.00014   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTqR_-VVaW_i"
      },
      "source": [
        "We obtain a reconstruction of the image with a high quality for $400$ hidden units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHN_4U7TauCT"
      },
      "source": [
        "## The datasets MNIST and USPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96-DpGQawy4"
      },
      "source": [
        "Those datasets are both 8-bit images in shades of grey representing numbers between 0 and 9. As the MNIST dataset contains $7000$ samples by class and USPS contains $1100$ samples by class, we randomly select $250$ samples for each dataset MNIST and USPS "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-5WXv4Sbm-4"
      },
      "source": [
        "## Conclusions for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUqjhwdbbpSU"
      },
      "source": [
        "The training for MNIST dataset has a higher computational time than the trainings over the two other datasets. This can be explained by the difference in the dimension of the input layer, which is greater for MNIST dataset.\n",
        "Those dimensions are respectively $28*28 = 784$ for MNIST, $20*16 = 320$ for Binary Alpha Digits and $16*16=256$ for USPS. Therefore, we reduce the number of epochs for the training, which requires several minutes for MNIST dataset but few seconds for Binary Alpha Digits dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ogz1MMMdgMe"
      },
      "source": [
        "We fail to reach the same quality of reconstruction than the one obtained with previous dataset, even if we rise the number of hidden units to $1000$. We suceed to obtain a satisfactory result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vCG0SCUebKX"
      },
      "source": [
        "## Conclusions for USPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH_fB1FTegKX"
      },
      "source": [
        "The learning is fast but the reconstruction of the dataset fails, even if we rise the number of hidden units to $1000$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvUfCMLveveW"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE_S9RoBexE-"
      },
      "source": [
        "The learning on Restricted Boltzmann Machines is faster than the training on a fully-connected neural network, it enables to reconstruct images faster. However, we have also seen that the performances are not satisfactory enough. For a simple dataset often use to test performances of a learning algorithm such as MNIST, the algorithm fails to reconstruct the images. Also, the Restricted Boltzmann Machine needs an important number of hidden units, where a single layer of a variational autoencoder is capable of the same results.\n",
        "\n",
        "\n",
        "Restricted Boltzmann Machines are less used in companies because other models performs better, such as feed-forward networks like variational autoencoders or generative adversarial networks. Some industries continue to use those algorithms as they succeed to show high performances, like in vocal recognition."
      ]
    }
  ]
}